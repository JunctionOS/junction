From b37c469fbc18b65c74fea1097d4ef626ed593474 Mon Sep 17 00:00:00 2001
From: Josh Fried <joshuafried@gmail.com>
Date: Tue, 14 Mar 2023 10:23:33 -0400
Subject: [PATCH 10/35] support external polling for tcp/udp sockets

---
 inc/runtime/poll.h   | 23 ++++++++++++
 inc/runtime/tcp.h    |  7 ++++
 inc/runtime/udp.h    |  3 ++
 runtime/net/tcp.c    | 83 ++++++++++++++++++++++++++++++++++++++++++++
 runtime/net/tcp.h    |  2 ++
 runtime/net/tcp_in.c | 11 ++++--
 runtime/net/udp.c    | 70 +++++++++++++++++++++++++++++++++----
 7 files changed, 191 insertions(+), 8 deletions(-)

diff --git a/inc/runtime/poll.h b/inc/runtime/poll.h
index 8658690d..86046ef4 100644
--- a/inc/runtime/poll.h
+++ b/inc/runtime/poll.h
@@ -9,6 +9,29 @@
 #include <runtime/thread.h>
 #include <runtime/sync.h>
 
+// External Poll Support
+
+typedef void (*poll_notif_fn_t)(unsigned long pdata, unsigned int event_mask);
+
+typedef struct poll_source {
+	poll_notif_fn_t set_fn;
+	poll_notif_fn_t clear_fn;
+	unsigned long poller_data;
+} poll_source_t;
+
+static inline void poll_clear(poll_source_t *src, unsigned int event_mask)
+{
+	if (src->clear_fn)
+		src->clear_fn(src->poller_data, event_mask);
+}
+
+static inline void poll_set(poll_source_t *src, unsigned int event_mask)
+{
+	if (src->set_fn)
+		src->set_fn(src->poller_data, event_mask);
+}
+
+
 typedef struct poll_waiter {
 	spinlock_t		lock;
 	struct list_head	triggered;
diff --git a/inc/runtime/tcp.h b/inc/runtime/tcp.h
index 338fdf34..4d01ca24 100644
--- a/inc/runtime/tcp.h
+++ b/inc/runtime/tcp.h
@@ -5,6 +5,7 @@
 #pragma once
 
 #include <runtime/net.h>
+#include <runtime/poll.h>
 #include <sys/uio.h>
 #include <sys/socket.h>
 
@@ -41,3 +42,9 @@ extern ssize_t tcp_writev(tcpconn_t *c, const struct iovec *iov, int iovcnt);
 extern int tcp_shutdown(tcpconn_t *c, int how);
 extern void tcp_abort(tcpconn_t *c);
 extern void tcp_close(tcpconn_t *c);
+
+extern void tcp_poll_install_cb(tcpconn_t *c, poll_notif_fn_t setfn,
+			                    poll_notif_fn_t clearfn, unsigned long data);
+extern void tcpq_poll_install_cb(tcpqueue_t *q, poll_notif_fn_t setfn,
+			                    poll_notif_fn_t clearfn, unsigned long data);
+
diff --git a/inc/runtime/udp.h b/inc/runtime/udp.h
index 1e15c460..2aa84efa 100644
--- a/inc/runtime/udp.h
+++ b/inc/runtime/udp.h
@@ -9,6 +9,7 @@
 #include <net/ip.h>
 #include <net/udp.h>
 #include <runtime/net.h>
+#include <runtime/poll.h>
 #include <sys/uio.h>
 
 /* the maximum possible payload size (for the largest possible MTU) */
@@ -49,6 +50,8 @@ extern void udp_shutdown(udpconn_t *c);
 extern void udp_close(udpconn_t *c);
 
 extern void udp_set_nonblocking(udpconn_t *c, bool nonblocking);
+extern void udp_poll_install_cb(udpconn_t *c, poll_notif_fn_t setfn,
+			                    poll_notif_fn_t clearfn, unsigned long data);
 
 /*
  * UDP Parallel API
diff --git a/runtime/net/tcp.c b/runtime/net/tcp.c
index c3104afe..32bca489 100644
--- a/runtime/net/tcp.c
+++ b/runtime/net/tcp.c
@@ -3,6 +3,7 @@
  */
 
 #include <string.h>
+#include <poll.h>
 
 #include <base/stddef.h>
 #include <base/hash.h>
@@ -231,6 +232,7 @@ void tcp_conn_set_state(tcpconn_t *c, int new_state)
 	if (c->pcb.state < TCP_STATE_ESTABLISHED &&
 	    new_state >= TCP_STATE_ESTABLISHED) {
 		waitq_release(&c->tx_wq);
+		poll_set(&c->poll_src, POLLOUT);
 	}
 
 	tcp_debug_state_change(c, c->pcb.state, new_state);
@@ -341,6 +343,9 @@ tcpconn_t *tcp_conn_alloc(void)
 	c->pcb.rcv_wnd = TCP_WIN;
 	c->pcb.rcv_mss = tcp_calculate_mss(net_get_mtu());
 
+	c->poll_src.set_fn = NULL;
+	c->poll_src.clear_fn = NULL;
+
 	return c;
 }
 
@@ -435,6 +440,8 @@ struct tcpqueue {
 	bool			shutdown;
 	bool			nonblocking;
 
+	poll_source_t	poll_src;
+
 	struct kref ref;
 	struct flow_registration flow;
 };
@@ -477,6 +484,7 @@ static void tcp_queue_recv(struct trans_entry *e, struct mbuf *m)
 	spin_lock_np(&q->l);
 	list_add_tail(&q->conns, &c->queue_link);
 	th = waitq_signal(&q->wq, &q->l);
+	poll_set(&q->poll_src, POLLIN);
 	spin_unlock_np(&q->l);
 	waitq_signal_finish(th);
 
@@ -536,6 +544,9 @@ int tcp_listen(struct netaddr laddr, int backlog, tcpqueue_t **q_out)
 	q->nonblocking = false;
 	kref_init(&q->ref);
 
+	q->poll_src.set_fn = NULL;
+	q->poll_src.clear_fn = NULL;
+
 
 	if (laddr.port == 0)
 		ret = trans_table_add_with_ephemeral_port(&q->e);
@@ -588,6 +599,10 @@ int tcp_accept(tcpqueue_t *q, tcpconn_t **c_out)
 	q->backlog++;
 	c = list_pop(&q->conns, tcpconn_t, queue_link);
 	assert(c != NULL);
+
+	if (list_empty(&q->conns) && !q->shutdown)
+		poll_clear(&q->poll_src, POLLIN);
+
 	spin_unlock_np(&q->l);
 
 	*c_out = c;
@@ -600,6 +615,7 @@ static void __tcp_qshutdown(tcpqueue_t *q)
 	spin_lock_np(&q->l);
 	BUG_ON(q->shutdown);
 	q->shutdown = true;
+	poll_set(&q->poll_src, POLLRDHUP | POLLHUP | POLLIN);
 	spin_unlock_np(&q->l);
 
 	/* prevent ingress receive and error dispatch (after RCU period) */
@@ -645,6 +661,9 @@ void tcp_qclose(tcpqueue_t *q)
 		tcp_close(c);
 	}
 
+	q->poll_src.set_fn = NULL;
+	q->poll_src.clear_fn = NULL;
+
 	kref_put(&q->ref, tcp_queue_release_ref);
 }
 
@@ -888,6 +907,10 @@ static ssize_t tcp_read_wait(tcpconn_t *c, size_t len,
 		      c->tx_last_ack + c->tx_last_win + c->winmax / 4)) {
 		do_ack = true;
 	}
+
+	if (list_empty(&c->rxq))
+		poll_clear(&c->poll_src, POLLIN);
+
 	spin_unlock_np(&c->lock);
 
 	if (do_ack)
@@ -1117,6 +1140,10 @@ static void tcp_write_finish(tcpconn_t *c)
 
 	tcp_timer_update(c);
 	waitq_release_start(&c->tx_wq, &waiters);
+
+	if (tcp_is_snd_full(c))
+		poll_clear(&c->poll_src, POLLOUT);
+
 	spin_unlock_np(&c->lock);
 
 	tcp_tx_fast_retransmit_finish(c, retransmit);
@@ -1232,6 +1259,7 @@ void tcp_conn_fail(tcpconn_t *c, int err)
 	if (!c->tx_closed) {
 		store_release(&c->tx_closed, true);
 		waitq_release(&c->tx_wq);
+		poll_set(&c->poll_src, POLLHUP);
 	}
 
 	/* will be freed by the writer if one is busy */
@@ -1264,6 +1292,7 @@ void tcp_conn_shutdown_rx(tcpconn_t *c)
 	if (c->rx_closed)
 		return;
 
+	poll_set(&c->poll_src, POLLRDHUP | POLLIN);
 	c->rx_closed = true;
 	waitq_release(&c->rx_wq);
 }
@@ -1297,6 +1326,7 @@ static int tcp_conn_shutdown_tx(tcpconn_t *c)
 	else
 		WARN();
 
+	poll_set(&c->poll_src, POLLHUP);
 	c->tx_closed = true;
 	waitq_release(&c->tx_wq);
 
@@ -1381,6 +1411,10 @@ void tcp_close(tcpconn_t *c)
 	if (ret)
 		tcp_conn_fail(c, -ret);
 	tcp_conn_shutdown_rx(c);
+
+	c->poll_src.set_fn = NULL;
+	c->poll_src.clear_fn = NULL;
+
 	spin_unlock_np(&c->lock);
 
 	tcp_conn_put(c);
@@ -1406,6 +1440,55 @@ void tcpq_set_nonblocking(tcpqueue_t *q, bool nonblocking)
 	spin_unlock_np(&q->l);
 }
 
+
+void tcp_poll_install_cb(tcpconn_t *c, poll_notif_fn_t setfn,
+			                    poll_notif_fn_t clearfn, unsigned long data)
+{
+	unsigned int flags = 0;
+
+	spin_lock_np(&c->lock);
+	c->poll_src.set_fn = setfn;
+	c->poll_src.clear_fn = clearfn;
+	c->poll_src.poller_data = data;
+
+	if (c->pcb.state == TCP_STATE_ESTABLISHED && !tcp_is_snd_full(c))
+		flags |= POLLOUT;
+
+	if (!list_empty(&c->rxq))
+		flags |= POLLIN;
+
+	if (c->rx_closed)
+		flags |= POLLRDHUP;
+
+	if (c->tx_closed)
+		flags |= POLLHUP;
+
+	poll_set(&c->poll_src, flags);
+
+	spin_unlock_np(&c->lock);
+}
+
+void tcpq_poll_install_cb(tcpqueue_t *q, poll_notif_fn_t setfn,
+			                    poll_notif_fn_t clearfn, unsigned long data)
+{
+	unsigned int flags = 0;
+
+	spin_lock_np(&q->l);
+	q->poll_src.set_fn = setfn;
+	q->poll_src.clear_fn = clearfn;
+	q->poll_src.poller_data = data;
+
+	if (!list_empty(&q->conns))
+		flags |= POLLIN;
+
+	if (q->shutdown)
+		flags |= POLLIN | POLLRDHUP | POLLHUP;
+
+	poll_set(&q->poll_src, flags);
+
+	spin_unlock_np(&q->l);
+}
+
 /**
  * tcp_init_late - starts the TCP worker thread
  *
diff --git a/runtime/net/tcp.h b/runtime/net/tcp.h
index e9fd3ef3..625281bd 100644
--- a/runtime/net/tcp.h
+++ b/runtime/net/tcp.h
@@ -92,6 +92,8 @@ struct tcpconn {
 	int			err; /* error code for read(), write(), etc. */
 	uint32_t		winmax; /* initial receive window size */
 
+	poll_source_t	poll_src;
+
 	/* ingress path */
 	bool			rx_closed;
 	bool			rx_exclusive;
diff --git a/runtime/net/tcp_in.c b/runtime/net/tcp_in.c
index d3fac688..0499c1d9 100644
--- a/runtime/net/tcp_in.c
+++ b/runtime/net/tcp_in.c
@@ -7,6 +7,8 @@
  * RX queue.
  */
 
+#include <poll.h>
+
 #include <base/stddef.h>
 #include <runtime/smalloc.h>
 #include <net/ip.h>
@@ -266,8 +268,10 @@ void tcp_rx_conn(struct trans_entry *e, struct mbuf *m)
 	store_release(&c->pcb.rcv_nxt_wnd, nxt_wnd);
 
 	/* should we wake a thread */
-	if (!list_empty(&c->rxq) || (tcphdr->flags & TCP_PUSH) > 0)
+	if (!list_empty(&c->rxq) || (tcphdr->flags & TCP_PUSH) > 0) {
 		rx_th = waitq_signal(&c->rx_wq, &c->lock);
+		poll_set(&c->poll_src, POLLIN);
+	}
 
 	/* handle delayed acks */
 	if (++c->acks_delayed_cnt >= 2) {
@@ -492,8 +496,10 @@ __tcp_rx_conn(tcpconn_t *c, struct mbuf *m, uint32_t ack, uint32_t snd_nxt,
 		do_ack = true;
 		goto done;
 	}
-	if (snd_was_full && !tcp_is_snd_full(c))
+	if (snd_was_full && !tcp_is_snd_full(c)) {
+		poll_set(&c->poll_src, POLLOUT);
 		waitq_release_start(&c->tx_wq, &waiters);
+	}
 
 	/*
 	 * Fast retransmit -> detect a duplicate ACK if:
@@ -554,6 +560,7 @@ __tcp_rx_conn(tcpconn_t *c, struct mbuf *m, uint32_t ack, uint32_t snd_nxt,
 			assert(!list_empty(&c->rxq));
 			assert(do_drop == false);
 			rx_th = waitq_signal(&c->rx_wq, &c->lock);
+			poll_set(&c->poll_src, POLLIN);
 		}
 		if (++c->acks_delayed_cnt >= 2) {
 			do_ack = true;
diff --git a/runtime/net/udp.c b/runtime/net/udp.c
index 13a9a990..fce0ab8c 100644
--- a/runtime/net/udp.c
+++ b/runtime/net/udp.c
@@ -3,6 +3,7 @@
  */
 
 #include <string.h>
+#include <poll.h>
 
 #include <base/hash.h>
 #include <base/kref.h>
@@ -71,6 +72,9 @@ struct udpconn {
 	int			outq_len;
 	waitq_t			outq_wq;
 
+	/* protected by @inq_lock (less likely that POLLOUT will be cleared) */
+	poll_source_t		poll_src;
+
 	struct kref		ref;
 	struct flow_registration		flow;
 };
@@ -96,7 +100,8 @@ static void udp_conn_recv(struct trans_entry *e, struct mbuf *m)
 
 	/* enqueue the packet on the ingress queue */
 	mbufq_push_tail(&c->inq, m);
-	c->inq_len++;
+	if (c->inq_len++ == 0)
+		poll_set(&c->poll_src, POLLIN);
 
 	/* wake up a waiter */
 	th = waitq_signal(&c->inq_wq, &c->inq_lock);
@@ -115,6 +120,10 @@ static void udp_conn_err(struct trans_entry *e, int err)
 	spin_lock_np(&c->inq_lock);
 	do_release = !c->inq_err && !c->shutdown;
 	c->inq_err = err;
+
+	if (do_release)
+		poll_set(&c->poll_src, POLLERR);
+
 	spin_unlock_np(&c->inq_lock);
 
 	if (do_release)
@@ -148,6 +157,9 @@ static void udp_init_conn(udpconn_t *c)
 	waitq_init(&c->outq_wq);
 
 	kref_init(&c->ref);
+
+	c->poll_src.set_fn = NULL;
+	c->poll_src.clear_fn = NULL;
 }
 
 static void udp_finish_release_conn(struct rcu_head *h)
@@ -290,6 +302,15 @@ int udp_set_buffers(udpconn_t *c, int read_mbufs, int write_mbufs)
 	c->inq_cap = read_mbufs;
 	c->outq_cap = write_mbufs;
 
+	spin_lock_np(&c->inq_lock);
+
+	if (c->outq_len < c->outq_cap)
+		poll_set(&c->poll_src, POLLOUT);
+	else
+		poll_clear(&c->poll_src, POLLOUT);
+
+	spin_unlock_np(&c->inq_lock);
+
 	/* TODO: free mbufs that go over new limits? */
 	return 0;
 }
@@ -338,7 +359,8 @@ ssize_t udp_read_from(udpconn_t *c, void *buf, size_t len,
 
 	/* pop an mbuf and deliver the payload */
 	m = mbufq_pop_head(&c->inq);
-	c->inq_len--;
+	if (--c->inq_len == 0 && !c->shutdown)
+		poll_clear(&c->poll_src, POLLIN);
 	spin_unlock_np(&c->inq_lock);
 
 	ret = MIN(len, mbuf_length(m));
@@ -364,7 +386,11 @@ static void udp_tx_release_mbuf(struct mbuf *m)
 	bool free_conn;
 
 	spin_lock_np(&c->outq_lock);
-	c->outq_len--;
+	if (c->outq_len-- == c->outq_cap) {
+		spin_lock(&c->inq_lock);
+		poll_set(&c->poll_src, POLLOUT);
+		spin_unlock(&c->inq_lock);
+	}
 	free_conn = (c->outq_free && c->outq_len == 0);
 	if (!c->shutdown)
 		th = waitq_signal(&c->outq_wq, &c->outq_lock);
@@ -427,7 +453,11 @@ ssize_t udp_write_to(udpconn_t *c, const void *buf, size_t len,
 		return -EPIPE;
 	}
 
-	c->outq_len++;
+	if (++c->outq_len >= c->outq_cap) {
+		spin_lock(&c->inq_lock);
+		poll_clear(&c->poll_src, POLLOUT);
+		spin_unlock(&c->inq_lock);
+	}
 	spin_unlock_np(&c->outq_lock);
 
 	m = net_tx_alloc_mbuf_sz(udp_headroom(), len);
@@ -487,12 +517,13 @@ ssize_t udp_write(udpconn_t *c, const void *buf, size_t len)
 
 static void __udp_shutdown(udpconn_t *c)
 {
-	spin_lock_np(&c->inq_lock);
 	spin_lock_np(&c->outq_lock);
+	spin_lock(&c->inq_lock);
 	BUG_ON(c->shutdown);
 	c->shutdown = true;
+	poll_set(&c->poll_src, POLLIN | POLLHUP | POLLRDHUP);
+	spin_unlock(&c->inq_lock);
 	spin_unlock_np(&c->outq_lock);
-	spin_unlock_np(&c->inq_lock);
 
 	/* prevent ingress receive and error dispatch (after RCU period) */
 	trans_table_remove(&c->e);
@@ -540,6 +571,9 @@ void udp_close(udpconn_t *c)
 		mbuf_free(m);
 	}
 
+	c->poll_src.set_fn = NULL;
+	c->poll_src.clear_fn = NULL;
+
 	spin_lock_np(&c->outq_lock);
 	free_conn = c->outq_len == 0;
 	c->outq_free = true;
@@ -565,6 +599,30 @@ void udp_set_nonblocking(udpconn_t *c, bool nonblocking)
 	spin_unlock_np(&c->outq_lock);
 }
 
+void udp_poll_install_cb(udpconn_t *c, poll_notif_fn_t setfn,
+			                    poll_notif_fn_t clearfn, unsigned long data)
+{
+	unsigned int flags = 0;
+
+	spin_lock_np(&c->inq_lock);
+	c->poll_src.set_fn = setfn;
+	c->poll_src.clear_fn = clearfn;
+	c->poll_src.poller_data = data;
+
+	if (c->outq_len < c->outq_cap)
+		flags |= POLLOUT;
+
+	if (c->inq_len)
+		flags |= POLLIN;
+
+	if (c->shutdown)
+		flags |= POLLHUP | POLLRDHUP;
+
+	poll_set(&c->poll_src, flags);
+
+	spin_unlock_np(&c->inq_lock);
+}
+
 
 /*
  * Parallel API
-- 
2.43.0

