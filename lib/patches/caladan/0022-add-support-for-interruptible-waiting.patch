From e50beaacba9e93a587eb2e81c45ef0b205c35da3 Mon Sep 17 00:00:00 2001
From: Josh Fried <joshuafried@gmail.com>
Date: Thu, 28 Sep 2023 22:57:58 +0000
Subject: [PATCH 22/35] add support for interruptible waiting

---
 inc/base/atomic.h                | 30 ++++++++++
 inc/base/types.h                 |  4 ++
 inc/runtime/interruptible_wait.h | 98 ++++++++++++++++++++++++++++++++
 inc/runtime/thread.h             | 49 +++++-----------
 inc/runtime/timer.h              |  2 +
 runtime/defs.h                   |  4 +-
 runtime/sched.c                  | 38 ++++++++++---
 runtime/switch.S                 |  2 +-
 runtime/timer.c                  | 45 ++++++++++++++-
 9 files changed, 224 insertions(+), 48 deletions(-)
 create mode 100644 inc/runtime/interruptible_wait.h

diff --git a/inc/base/atomic.h b/inc/base/atomic.h
index 4c2aefb9..a4504e6c 100644
--- a/inc/base/atomic.h
+++ b/inc/base/atomic.h
@@ -17,16 +17,36 @@ static inline int atomic_read(const atomic_t *a)
 	return *((volatile int *)&a->cnt);
 }
 
+static inline int8_t atomic8_read(const atomic8_t *a)
+{
+	return *((volatile int8_t *)&a->cnt);
+}
+
 static inline void atomic_write(atomic_t *a, int val)
 {
 	a->cnt = val;
 }
 
+static inline void atomic8_write(atomic8_t *a, int8_t val)
+{
+	a->cnt = val;
+}
+
 static inline int atomic_fetch_and_add(atomic_t *a, int val)
 {
 	return __sync_fetch_and_add(&a->cnt, val);
 }
 
+static inline int atomic_fetch_and_add_relaxed(atomic_t *a, int val)
+{
+	return __atomic_fetch_add(&a->cnt, val, __ATOMIC_RELAXED);
+}
+
+static inline int8_t atomic8_fetch_and_add_relaxed(atomic8_t *a, int8_t val)
+{
+	return __atomic_fetch_add(&a->cnt, val, __ATOMIC_RELAXED);
+}
+
 static inline int atomic_fetch_and_sub(atomic_t *a, int val)
 {
 	return __sync_fetch_and_sub(&a->cnt, val);
@@ -57,6 +77,16 @@ static inline int atomic_sub_and_fetch(atomic_t *a, int val)
 	return __sync_sub_and_fetch(&a->cnt, val);
 }
 
+static inline int atomic_sub_and_fetch_relaxed(atomic_t *a, int val)
+{
+	return __atomic_sub_fetch(&a->cnt, val, __ATOMIC_RELAXED);
+}
+
+static inline int8_t atomic8_sub_and_fetch_relaxed(atomic8_t *a, int8_t val)
+{
+	return __atomic_sub_fetch(&a->cnt, val, __ATOMIC_RELAXED);
+}
+
 static inline void atomic_inc(atomic_t *a)
 {
 	atomic_fetch_and_add(a, 1);
diff --git a/inc/base/types.h b/inc/base/types.h
index 10a58940..338c9be7 100644
--- a/inc/base/types.h
+++ b/inc/base/types.h
@@ -41,6 +41,10 @@ typedef struct {
 	volatile int locked;
 } spinlock_t;
 
+typedef struct {
+	volatile int8_t cnt;
+} atomic8_t;
+
 typedef struct {
 	volatile int cnt;
 } atomic_t;
diff --git a/inc/runtime/interruptible_wait.h b/inc/runtime/interruptible_wait.h
new file mode 100644
index 00000000..9120297f
--- /dev/null
+++ b/inc/runtime/interruptible_wait.h
@@ -0,0 +1,98 @@
+/*
+ * interruptible_wait.h - support for interrupting blocked threads
+ */
+
+#pragma once
+
+#include <base/list.h>
+#include <base/lock.h>
+#include <runtime/thread.h>
+
+#define PREPARED_FLAG		(1U << 6)
+#define PREPARED_MASK		(PREPARED_FLAG - 1)
+#define WAKER_VAL			(1 + PREPARED_FLAG)
+
+// Returns true if this thread was interrupted.
+// @th must be thread_self().
+static inline bool prepare_interruptible(thread_t *th)
+{
+	assert(th == thread_self());
+	return atomic8_fetch_and_add_relaxed(&th->interrupt_state, WAKER_VAL) > 0;
+}
+
+// Called after enqueuing a signal to set the interrupt flag.
+// Can only be called once, must be synchronized with signal lock.
+static inline bool deliver_interrupt(thread_t *th)
+{
+	if (atomic8_fetch_and_add_relaxed(&th->interrupt_state, 1) > 0) {
+		thread_ready(th);
+		return true;
+	}
+
+	return false;
+}
+
+// Returns interrupt state.
+// After wakeup (must be synchronized using waker lock):
+// if state > 1: thread was interrupted, needs to be disarmed
+// else if state > 0: both interrupt and wake occurred, no disarm needed
+// if state == 0: thread was woken normally, no interrupt.
+static inline int get_interruptible_status(const thread_t *th)
+{
+	return atomic8_read(&th->interrupt_state) & PREPARED_MASK;
+}
+
+// Wake a thread that is blocked pending a wake or interrupt.
+// This thread must have called prepare_interruptible().
+// Must have previously synchronized with a waker lock.
+static inline void interruptible_wake_prepared(thread_t *th)
+{
+	if (atomic8_sub_and_fetch_relaxed(&th->interrupt_state, WAKER_VAL) == 0)
+		thread_ready(th);
+}
+
+// Check if a thread was prepared to receive interrupts.
+// This can only be called by a waker, which must have previously synchronized
+// with a waker lock.
+static inline bool check_prepared(const thread_t *th) {
+	return (atomic8_read(&th->interrupt_state) & PREPARED_FLAG) != 0;
+}
+
+// Test whether or not a non-interrupt waker should call thread_ready
+static inline bool interruptible_wake_test(thread_t *th)
+{
+	return !check_prepared(th) ||
+	        atomic8_sub_and_fetch_relaxed(&th->interrupt_state, WAKER_VAL) == 0;
+}
+
+// Wake a thread that is blocked.
+// The thread does not need to have been armed with prepare_interruptible().
+// If the caller is certain that this thread was armed, it can call
+// interruptible_wake_prepared() directly.
+// Must have previously synchronized with a waker lock.
+static inline void interruptible_wake(thread_t *th)
+{
+	if (interruptible_wake_test(th))
+		thread_ready(th);
+}
+
+// Must be called with signal lock to synchronize with interrupt delivery.
+static inline void reset_interruptible_state(thread_t *th)
+{
+	atomic8_write(&th->interrupt_state, 0);
+}
+
+// Mark this thread interrupted, typically used when the blocked signal mask
+// is updated. Caller should synchronize with signal lock.
+static inline void set_interrupt_state_interrupted()
+{
+	atomic8_write(&thread_self()->interrupt_state, 1);
+}
+
+// Check if a thread was interrupted.
+// @th must be thread_self().
+static inline bool thread_interrupted(const thread_t *th)
+{
+	assert(th == thread_self());
+	return atomic8_read(&th->interrupt_state) > 0;
+}
diff --git a/inc/runtime/thread.h b/inc/runtime/thread.h
index 9f5231ac..07557617 100644
--- a/inc/runtime/thread.h
+++ b/inc/runtime/thread.h
@@ -57,49 +57,30 @@ static inline thread_t *thread_self(void)
 struct stack;
 
 struct thread {
-    struct thread_tf    tf;
-    struct list_node    link;
+    bool        main_thread:1;
+    bool        has_fsbase:1;
+    bool        thread_ready:1;
+    bool        junction_thread;
+    bool        thread_running;
+    bool        in_syscall;
+    atomic8_t        interrupt_state;
+    struct thread_tf    *entry_regs;
+    unsigned long    junction_tstate_buf[20];
     struct stack        *stack;
-    unsigned int        main_thread:1;
-    unsigned int        has_fsbase:1;
-    unsigned int        thread_ready;
-    unsigned int        thread_running;
-    unsigned int        last_cpu;
-    uint64_t        run_start_tsc;
+    uint16_t        last_cpu;
+    uint16_t        cur_kthread;
     uint64_t        ready_tsc;
+    struct thread_tf    tf;
     uint64_t		fsbase;
-    uint64_t        tlsvar;
-     // Trapframe used by junction to stash registers on syscall entry
-    struct thread_tf	junction_tf;
-    unsigned long    junction_tstate_buf[24];
+    struct list_node    link;
+    struct list_node    interruptible_link;
+    bool        link_armed;
 #ifdef GC
     struct list_node    gc_link;
     unsigned int        onk;
 #endif
 };
 
-
-static inline uint64_t __get_uthread_specific(thread_t *th)
-{
-    return th->tlsvar;
-}
-
-static inline void __set_uthread_specific(thread_t *th, uint64_t val)
-{
-    th->tlsvar = val;
-}
-
-static inline uint64_t get_uthread_specific(void)
-{
-    return thread_self()->tlsvar;
-}
-
-static inline void set_uthread_specific(uint64_t val)
-{
-    thread_self()->tlsvar = val;
-}
-
-
 /*
  * High-level routines, use this API most of the time.
  */
diff --git a/inc/runtime/timer.h b/inc/runtime/timer.h
index b5ea331e..28cb3b03 100644
--- a/inc/runtime/timer.h
+++ b/inc/runtime/timer.h
@@ -92,3 +92,5 @@ extern bool timer_cancel_recurring(struct timer_entry *e);
 
 extern void timer_sleep_until(uint64_t deadline_us);
 extern void timer_sleep(uint64_t duration_us);
+extern void __timer_sleep_interruptible(uint64_t deadline_us);
+extern void timer_sleep_interruptible(uint64_t duration_us);
\ No newline at end of file
diff --git a/runtime/defs.h b/runtime/defs.h
index 17ee6dd5..847e0416 100644
--- a/runtime/defs.h
+++ b/runtime/defs.h
@@ -48,10 +48,10 @@
 typedef void (*runtime_fn_t)(void);
 
 /* assembly helper routines from switch.S */
-extern void __jmp_thread(struct thread_tf *tf) __noreturn;
+extern void __jmp_thread(struct thread_tf *tf);
 extern void __jmp_thread_direct(struct thread_tf *oldtf,
 				struct thread_tf *newtf,
-				unsigned int *thread_running);
+				bool *thread_running);
 extern void __jmp_runtime(struct thread_tf *tf, runtime_fn_t fn,
 			  void *stack);
 extern void __jmp_runtime_nosave(runtime_fn_t fn, void *stack) __noreturn;
diff --git a/runtime/sched.c b/runtime/sched.c
index 82c73193..db3541de 100644
--- a/runtime/sched.c
+++ b/runtime/sched.c
@@ -16,6 +16,7 @@
 #include <base/log.h>
 #include <runtime/sync.h>
 #include <runtime/thread.h>
+#include <runtime/interruptible_wait.h>
 
 #include "defs.h"
 
@@ -40,6 +41,9 @@ static DEFINE_PERTHREAD(struct tcache_perthread, thread_pt);
 /* used to track cycle usage in scheduler */
 static DEFINE_PERTHREAD(uint64_t, last_tsc);
 
+// Junction overrides this function.
+void __weak on_sched(thread_t *th) {}
+
 /**
  * In inc/runtime/thread.h, this function is declared inline (rather than static
  * inline) so that it is accessible to the Rust bindings. As a result, it must
@@ -74,10 +78,10 @@ static inline bool cores_have_affinity(unsigned int cpua, unsigned int cpub)
  * This function restores the state of the thread and switches from the runtime
  * stack to the thread's stack. Runtime state is not saved.
  */
-static __noreturn void jmp_thread(thread_t *th)
+static void jmp_thread(thread_t *th)
 {
 	assert_preempt_disabled();
-	assert(th->thread_ready);
+	assert(th->thread_ready == true);
 
 	perthread_store(__self, th);
 	th->thread_ready = false;
@@ -93,6 +97,10 @@ static __noreturn void jmp_thread(thread_t *th)
 	set_fsbase(th->fsbase);
 
 	th->thread_running = true;
+
+	if (th->junction_thread)
+		on_sched(th);
+
 	__jmp_thread(&th->tf);
 }
 
@@ -107,7 +115,7 @@ static __noreturn void jmp_thread(thread_t *th)
 static void jmp_thread_direct(thread_t *oldth, thread_t *newth)
 {
 	assert_preempt_disabled();
-	assert(newth->thread_ready);
+	assert(newth->thread_ready == true);
 
 	perthread_store(__self, newth);
 	newth->thread_ready = false;
@@ -123,6 +131,10 @@ static void jmp_thread_direct(thread_t *oldth, thread_t *newth)
 	set_fsbase(newth->fsbase);
 
 	newth->thread_running = true;
+
+	if (newth->junction_thread)
+		on_sched(newth);
+
 	__jmp_thread_direct(&oldth->tf, &newth->tf, &oldth->thread_running);
 }
 
@@ -330,7 +342,7 @@ static __noinline bool do_watchdog(struct kthread *l)
 }
 
 /* the main scheduler routine, decides what to run next */
-static __noreturn __noinline void schedule(void)
+static __noinline void schedule(void)
 {
 	struct kthread *r = NULL, *l = myk();
 	uint64_t start_tsc;
@@ -468,6 +480,9 @@ done:
 		drain_overflow(l);
 
 	update_oldest_tsc(l);
+
+	th->cur_kthread = l->kthread_idx;
+
 	spin_unlock(&l->lock);
 
 	/* update exit stat counters */
@@ -479,7 +494,6 @@ done:
 		STAT(REMOTE_RUNS)++;
 
 	/* update exported thread run start time */
-	th->run_start_tsc = perthread_get_stable(last_tsc);
 	ACCESS_ONCE(l->q_ptrs->run_start_tsc) = perthread_get_stable(last_tsc);
 
 	/* increment the RCU generation number (odd is in thread) */
@@ -505,6 +519,8 @@ static __always_inline void enter_schedule(thread_t *curth)
 	spin_lock(&k->lock);
 	now_tsc = rdtsc();
 
+	th = k->rq[k->rq_tail % RUNTIME_RQ_SIZE];
+
 	/* slow path: switch from the uthread stack to the runtime stack */
 	if (k->rq_head == k->rq_tail ||
 	    preempt_cede_needed(k) ||
@@ -523,7 +539,7 @@ static __always_inline void enter_schedule(thread_t *curth)
 	perthread_get_stable(last_tsc) = now_tsc;
 
 	/* pop the next runnable thread from the queue */
-	th = k->rq[k->rq_tail++ % RUNTIME_RQ_SIZE];
+	k->rq_tail++;
 	ACCESS_ONCE(k->q_ptrs->rq_tail)++;
 
 	/* move overflow tasks into the runqueue */
@@ -531,10 +547,11 @@ static __always_inline void enter_schedule(thread_t *curth)
 		drain_overflow(k);
 
 	update_oldest_tsc(k);
+	curth->cur_kthread = NCPU;
+	th->cur_kthread = k->kthread_idx;
 	spin_unlock(&k->lock);
 
 	/* update exported thread run start time */
-	th->run_start_tsc = perthread_get_stable(last_tsc);
 	ACCESS_ONCE(k->q_ptrs->run_start_tsc) = perthread_get_stable(last_tsc);
 
 	/* increment the RCU generation number (odd is in thread) */
@@ -840,7 +857,10 @@ static __always_inline thread_t *__thread_create(void)
 	th->has_fsbase = false;
 	th->thread_ready = false;
 	th->thread_running = false;
-	th->tlsvar = 0;
+	th->junction_thread = false;
+	th->link_armed = false;
+	th->cur_kthread = NCPU;
+	atomic8_write(&th->interrupt_state, 0);
 
 	return th;
 }
@@ -970,7 +990,7 @@ void thread_exit(void)
  * immediately park each kthread when it first starts up, only schedule it once
  * the iokernel has granted it a core
  */
-static __noreturn void schedule_start(void)
+static void schedule_start(void)
 {
 	struct kthread *k = myk();
 
diff --git a/runtime/switch.S b/runtime/switch.S
index 867cff95..568985f6 100644
--- a/runtime/switch.S
+++ b/runtime/switch.S
@@ -95,7 +95,7 @@ __jmp_thread_direct:
 	movq    RIP(%rsi), %r8
 
 	/* clear the stack busy flag */
-	movl	$0, (%rdx)
+	movb	$0, (%rdx)
 
 	/* restore callee regs */
 	movq    RBX(%rsi), %rbx
diff --git a/runtime/timer.c b/runtime/timer.c
index 9c3ae258..75d25b10 100644
--- a/runtime/timer.c
+++ b/runtime/timer.c
@@ -12,6 +12,7 @@
 #include <runtime/sync.h>
 #include <runtime/thread.h>
 #include <runtime/timer.h>
+#include <runtime/interruptible_wait.h>
 
 #include "defs.h"
 
@@ -268,8 +269,7 @@ static void __timer_sleep(uint64_t deadline_us)
 	timer_init(&e, timer_finish_sleep, (unsigned long)thread_self());
 
 	k = getk();
-	spin_lock_np(&k->timer_lock);
-	putk();
+	spin_lock(&k->timer_lock);
 	timer_start_locked(k, &e, deadline_us);
 	update_q_ptrs(k);
 	thread_park_and_unlock_np(&k->timer_lock);
@@ -277,6 +277,38 @@ static void __timer_sleep(uint64_t deadline_us)
 	timer_finish(&e);
 }
 
+static void timer_finish_interruptible_sleep(unsigned long arg)
+{
+	thread_t *th = (thread_t *)arg;
+	interruptible_wake_prepared(th);
+}
+
+
+void __timer_sleep_interruptible(uint64_t deadline_us)
+{
+	struct kthread *k;
+	struct timer_entry e;
+
+	thread_t *th = thread_self();
+
+	timer_init(&e, timer_finish_interruptible_sleep, (unsigned long)th);
+
+	k = getk();
+
+	spin_lock(&k->timer_lock);
+
+	if (prepare_interruptible(th)) {
+		spin_unlock_np(&k->timer_lock);
+		return;
+	}
+
+	timer_start_locked(k, &e, deadline_us);
+	update_q_ptrs(k);
+	thread_park_and_unlock_np(&k->timer_lock);
+
+	timer_cancel(&e);
+}
+
 /**
  * timer_sleep_until - sleeps until a deadline
  * @deadline_us: the deadline time in microseconds
@@ -298,6 +330,15 @@ void timer_sleep(uint64_t duration_us)
 	__timer_sleep(microtime() + duration_us);
 }
 
+/**
+ * timer_sleep - sleeps for a duration
+ * @duration_us: the duration time in microseconds
+ */
+void timer_sleep_interruptible(uint64_t duration_us)
+{
+	__timer_sleep_interruptible(microtime() + duration_us);
+}
+
 static void timer_softirq_one(struct kthread *k)
 {
 	struct timer_entry *e;
-- 
2.43.0

