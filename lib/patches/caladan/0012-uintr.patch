From 24247f09e6a2e3ea4f0eb5f315eeadffe2b3d268 Mon Sep 17 00:00:00 2001
From: Josh Fried <joshuafried@gmail.com>
Date: Sun, 31 Dec 2023 04:21:49 +0000
Subject: [PATCH 12/13] uintr

---
 inc/runtime/preempt.h |  1 +
 inc/runtime/thread.h  |  4 +++-
 runtime/defs.h        | 34 +++++++++++++++++++++++++++++++---
 runtime/init.c        |  2 +-
 runtime/preempt.c     | 17 ++++++++++++++++-
 runtime/sched.c       |  3 ++-
 runtime/stack.c       |  4 ++--
 7 files changed, 56 insertions(+), 9 deletions(-)

diff --git a/inc/runtime/preempt.h b/inc/runtime/preempt.h
index bc6e10f..6c3da71 100644
--- a/inc/runtime/preempt.h
+++ b/inc/runtime/preempt.h
@@ -13,6 +13,7 @@ DECLARE_PERTHREAD(void *, uintr_stack);
 extern void preempt(void);
 extern void uintr_asm_return(void);
 
+extern bool uintr_enabled;
 extern size_t xsave_max_size;
 extern size_t xsave_features;
 
diff --git a/inc/runtime/thread.h b/inc/runtime/thread.h
index 0b422c8..e560d1a 100644
--- a/inc/runtime/thread.h
+++ b/inc/runtime/thread.h
@@ -27,9 +27,11 @@ struct thread {
 	bool	junction_thread;
 	bool	thread_running;
 	bool	in_syscall;
+	/* modified by interrupt handler; should not be shared with other bitfields */
+	bool	xsave_area_in_use:1;
 	atomic8_t	interrupt_state;
 	struct thread_tf	*entry_regs;
-	unsigned long	junction_tstate_buf[20];
+	unsigned long	junction_tstate_buf[22];
 	struct stack	*stack;
 	uint16_t	last_cpu;
 	uint16_t	cur_kthread;
diff --git a/runtime/defs.h b/runtime/defs.h
index 3093af3..ed33f5c 100644
--- a/runtime/defs.h
+++ b/runtime/defs.h
@@ -39,6 +39,8 @@
 #define RUNTIME_WATCHDOG_US		50
 #define RUNTIME_RX_BATCH_SIZE		32
 
+#define XSAVE_AREA_SIZE (24 * KB)
+#define XSAVE_AREA_PTR_SIZE (XSAVE_AREA_SIZE / sizeof(uintptr_t))
 
 /*
  * Thread support
@@ -73,6 +75,21 @@ struct stack {
 DECLARE_PERTHREAD(struct tcache_perthread, stack_pt);
 DECLARE_PERTHREAD(void *, runtime_stack);
 
+static __always_inline void *stack_to_tcache_handle(struct stack *s)
+{
+	/*
+	 * use the bottom page of the stack (before the xsave area) for the tcache's
+	 * intrusive list. This way we don't fault in any more pages than we need.
+	 */
+	return (void *)((uintptr_t)(s + 1) - XSAVE_AREA_SIZE - PGSIZE_4KB);
+}
+
+static __always_inline struct stack *stack_from_tcache_handle(void *handle)
+{
+	uintptr_t addr = (uintptr_t)handle + PGSIZE_4KB + XSAVE_AREA_SIZE;
+	return (struct stack *)addr - 1;
+}
+
 /**
  * stack_alloc - allocates a stack
  *
@@ -85,7 +102,7 @@ static inline struct stack *stack_alloc(void)
 	void *s = tcache_alloc(perthread_ptr(stack_pt));
 	if (unlikely(!s))
 		return NULL;
-	return container_of((uintptr_t (*)[STACK_PTR_SIZE])s, struct stack, usable);
+	return stack_from_tcache_handle(s);
 }
 
 /**
@@ -94,7 +111,7 @@ static inline struct stack *stack_alloc(void)
  */
 static inline void stack_free(struct stack *s)
 {
-	tcache_free(perthread_ptr(stack_pt), (void *)s->usable);
+	tcache_free(perthread_ptr(stack_pt), stack_to_tcache_handle(s));
 }
 
 #define RSP_ALIGNMENT	16
@@ -122,6 +139,17 @@ static inline uint64_t stack_init_to_rsp(struct stack *s, void (*exit_fn)(void))
 {
 	uint64_t rsp;
 
+	s->usable[STACK_PTR_SIZE - XSAVE_AREA_PTR_SIZE - 1] = (uintptr_t)exit_fn;
+	rsp = (uint64_t)&s->usable[STACK_PTR_SIZE - XSAVE_AREA_PTR_SIZE - 1];
+	assert_rsp_aligned(rsp);
+	return rsp;
+}
+
+static inline uint64_t runtime_stack_init_to_rsp(struct stack *s,
+                                                 void (*exit_fn)(void))
+{
+	uint64_t rsp;
+
 	s->usable[STACK_PTR_SIZE - 1] = (uintptr_t)exit_fn;
 	rsp = (uint64_t)&s->usable[STACK_PTR_SIZE - 1];
 	assert_rsp_aligned(rsp);
@@ -142,7 +170,7 @@ static inline uint64_t
 stack_init_to_rsp_with_buf(struct stack *s, void **buf, size_t buf_len,
 			   void (*exit_fn)(void))
 {
-	uint64_t rsp, pos = STACK_PTR_SIZE;
+	uint64_t rsp, pos = STACK_PTR_SIZE - XSAVE_AREA_PTR_SIZE;
 
 	/* reserve the buffer */
 	pos -= div_up(buf_len, sizeof(uint64_t));
diff --git a/runtime/init.c b/runtime/init.c
index ae57dfb..aeb38b1 100644
--- a/runtime/init.c
+++ b/runtime/init.c
@@ -54,10 +54,10 @@ static const struct init_entry global_init_handlers[] = {
 /* per-kthread subsystem initialization */
 static const struct init_entry thread_init_handlers[] = {
 	/* runtime core */
+	THREAD_INITIALIZER(stack),
 	THREAD_INITIALIZER(preempt),
 	THREAD_INITIALIZER(kthread),
 	THREAD_INITIALIZER(ioqueues),
-	THREAD_INITIALIZER(stack),
 	THREAD_INITIALIZER(sched),
 	THREAD_INITIALIZER(timer),
 	THREAD_INITIALIZER(smalloc),
diff --git a/runtime/preempt.c b/runtime/preempt.c
index de0fd8b..7f955ea 100644
--- a/runtime/preempt.c
+++ b/runtime/preempt.c
@@ -26,6 +26,8 @@
 
 /* the current preemption count */
 DEFINE_PERTHREAD(unsigned int, preempt_cnt);
+/* whether uintr is enabled */
+bool uintr_enabled;
 /* perthread stack to use supply for UIPIs */
 DEFINE_PERTHREAD(void *, uintr_stack);
 /* maximum size in bytes needed for xsave */
@@ -166,8 +168,20 @@ void preempt(void)
 
 int preempt_init_thread(void)
 {
+	struct stack *s;
+	uint64_t stack_val;
+
 	perthread_store(preempt_cnt, PREEMPT_NOT_PENDING);
-	perthread_store(uintr_stack, (void *)REDZONE_SIZE);
+
+	if (!uintr_enabled)
+		return 0;
+
+	s = stack_alloc();
+	if (!s)
+		return -ENOMEM;
+
+	stack_val = ((uint64_t)&s->usable[STACK_PTR_SIZE]) | 1UL;
+	perthread_store(uintr_stack, (void *)stack_val);
 	return 0;
 }
 
@@ -212,6 +226,7 @@ int preempt_init(void)
 	}
 
 	log_info("uintr: enabled");
+	uintr_enabled = true;
 
 	ret = syscall(SYS_arch_prctl, ARCH_GET_XCOMP_SUPP, &xsave_features);
 	if (unlikely(ret)) {
diff --git a/runtime/sched.c b/runtime/sched.c
index 3e86d37..5e23292 100644
--- a/runtime/sched.c
+++ b/runtime/sched.c
@@ -860,6 +860,7 @@ static __always_inline thread_t *__thread_create(void)
 	// Can be used to detect newly created thread.
 	th->ready_tsc = 0;
 	th->total_cycles = 0;
+	th->xsave_area_in_use = false;
 	atomic8_write(&th->interrupt_state, 0);
 
 	return th;
@@ -1039,7 +1040,7 @@ int sched_init_thread(void)
 	if (!s)
 		return -ENOMEM;
 
-	perthread_store(runtime_stack, (void *)stack_init_to_rsp(s, runtime_top_of_stack));
+	perthread_store(runtime_stack, (void *)runtime_stack_init_to_rsp(s, runtime_top_of_stack));
 	perthread_store(runtime_fsbase, _readfsbase_u64());
 
 	ss.ss_sp = &s->usable[0];
diff --git a/runtime/stack.c b/runtime/stack.c
index b9fba77..97ce479 100644
--- a/runtime/stack.c
+++ b/runtime/stack.c
@@ -57,7 +57,7 @@ static void stack_tcache_free(struct tcache *tc, int nr, void **items)
 
 	/* try to release the backing memory first */
 	for (i = 0; i < nr; i++)
-		stack_reclaim(container_of(items[i], struct stack, usable));
+		stack_reclaim(stack_from_tcache_handle(items[i]));
 
 	/* then make the stacks available for reallocation */
 	spin_lock(&stack_lock);
@@ -87,7 +87,7 @@ static int stack_tcache_alloc(struct tcache *tc, int nr, void **items)
 		s = stack_create(base);
 		if (unlikely(!s))
 			goto fail;
-		items[i] = s->usable;
+		items[i] = stack_to_tcache_handle(s);
 	}
 
 	return 0;
-- 
2.43.0

